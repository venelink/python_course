{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Linguists\n",
    "\n",
    "Notebook 7: Introduction to NLTK\n",
    "\n",
    "Venelin Kovatchev\n",
    "\n",
    "University of Barcelona 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will start working with NLTK.\n",
    "\n",
    "We will load a corpus and try to process it.\n",
    "\n",
    "We will apply simple transformations like sentence segmentation and tokenization.\n",
    "\n",
    "We will calculate basic statistics using loops as well as nltk built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the important packages for today\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all resources needed for today\n",
    "from nltk.corpus import reuters\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora in NLTK\n",
    "\n",
    "NLTK contains many corpora for various languages\n",
    "\n",
    "We can access them with the IMPORT function\n",
    "\n",
    "Most corpora contains multiple files (e.g.: news articles, tweets, web pages, etc)\n",
    "\n",
    "You can get a list of all filenames and then work with one or more of the files\n",
    "\n",
    "You can also use the whole corpus by not specifying any filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the identifiers for all files in the reuters corpus\n",
    "print(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by working with just one file, 'test/14826'\n",
    "c_fid = 'test/14826'\n",
    "\n",
    "# Let's get the raw version of this file\n",
    "test_raw = reuters.raw(c_fid)\n",
    "\n",
    "# Let's see the test file\n",
    "# We can see that it doesn't contain any strange code or hashtags\n",
    "# All characters are readable, so we don't need to clean the file\n",
    "print(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is an important task in NLP and CL.\n",
    "\n",
    "During tokenization, we convert a sequencee of characters (a string) into a sequence of tokens (a list of strings).\n",
    "\n",
    "There are several ways to tokenize a corpus:\n",
    "\n",
    "- some corpora come pre-tokenized so we can just use the tokenized version\n",
    "- as we already saw in previous classes, the .split() method converts a string into a list, separating by whitespaces\n",
    "- some \"libraries\" such as NLTK have functions that can make a \"smarter\" tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the tokenized version of this file\n",
    "test_tok = reuters.words(c_fid)\n",
    "\n",
    "# Let's look at the first 30 characters\n",
    "print(test_tok[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the tokenized and sentence segmented version of this file\n",
    "test_sent = reuters.sents(c_fid)\n",
    "\n",
    "# Let's look at the first two sentences\n",
    "# Remember, the slicing excludes the last element, so to take first two we need 0:2\n",
    "print(test_sent[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has a tool that can automatically tokenize a corpus\n",
    "manual_tok = word_tokenize(test_raw)\n",
    "\n",
    "# Now let's use the split function to separate words\n",
    "split_tok = test_raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the three tokenizations next to each other\n",
    "def comp_tok(corp_1, corp_2, num_tok = 30):\n",
    "    # Let's zip the two corpora\n",
    "    mix_corp = zip(corp_1[0:num_tok],corp_2[0:num_tok])\n",
    "    # Let's go through them token by token\n",
    "    for word_1, word_2 in mix_corp:\n",
    "        # Print the two words\n",
    "        print(word_1,word_2)          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the pre-tokenized corpus with the automatically tokenized one\n",
    "comp_tok(manual_tok,test_tok,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare the automatically tokenized and the split\n",
    "comp_tok(manual_tok,split_tok,25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK also has a tool that autpmatically splits sentences\n",
    "# NLTK also has a tool that can automatically separate a corpus by sentences\n",
    "manual_sent = sent_tokenize(test_raw)\n",
    "\n",
    "# Observe the first two sentences\n",
    "print(manual_sent[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics \n",
    "\n",
    "Basic corpus statistics include things like\n",
    "\n",
    "- number of tokens\n",
    "- number of unique tokens (types)\n",
    "- the frequency of each token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the number of tokens in a list, we can use the \"len\" function\n",
    "\n",
    "num_tokens_1 = len(manual_tok)\n",
    "num_tokens_2 = len(test_tok)\n",
    "\n",
    "print(\"The pre-tokenized document has \" + str(num_tokens_2) + \" tokens\")\n",
    "print(\"The word_tokenize document has \" + str(num_tokens_1) + \" tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the number of types (unique tokens) we can either go element by element and count or we can use the set() command\n",
    "# A set is a special data type (similar to a list), where no duplicate elements are allowed\n",
    "\n",
    "# Let's  see how it works\n",
    "my_test_list = [\"some\",\"element\",\"some\",\"other\",\"element\"]\n",
    "print(my_test_list)\n",
    "my_test_set = set(my_test_list)\n",
    "print(my_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the set, we can easily see the number of unique tokens in the corpus\n",
    "num_utokens_1 = len(set(manual_tok))\n",
    "num_utokens_2 = len(set(test_tok))\n",
    "\n",
    "print(\"The pre-tokenized document has \" + str(num_utokens_2) + \" unique tokens\")\n",
    "print(\"The word_tokenize document has \" + str(num_utokens_1) + \" unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there is one more pre-processing step we can do here\n",
    "# In the corpus some words can be capitalized differently - \"Asia\" \"ASIA\", etc\n",
    "# Depending on the task, we might want to keep this distinction or not\n",
    "# If we want to ignore case, we need to \"lowercase\" all words\n",
    "\n",
    "l_manual_tok = [word.lower() for word in manual_tok]\n",
    "l_test_tok = [word.lower() for word in test_tok]\n",
    "\n",
    "# Remember the [X for X in ...]\n",
    "# this is the same as:\n",
    "# \n",
    "# l_manual_tok = []\n",
    "# for word in manual_tok:\n",
    "#     l_manual_tok.append(word.lower())\n",
    "\n",
    "num_ultokens_1 = len(set(l_manual_tok))\n",
    "num_ultokens_2 = len(set(l_test_tok))\n",
    "\n",
    "print(\"The pre-tokenized document has \" + str(num_ultokens_2) + \" unique lowercased tokens\")\n",
    "print(\"The word_tokenize document has \" + str(num_ultokens_1) + \" unique lowercased tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is counting the frequency of every word\n",
    "# We did this few classes ago by manually going through the list and adding the words in a dictionary\n",
    "# The following code is copied from the previous exercise\n",
    "\n",
    "# We start by creating the dictionary:\n",
    "freq_dict = {}\n",
    "\n",
    "# We loop through the manual_toc list\n",
    "for word in manual_tok:\n",
    "    # For every word in the tokenized corpus we check if it is in the dictionary\n",
    "    if word not in freq_dict:\n",
    "        # if it is NOT in the vocabulary, we add it with a default frequency of 1 (we just saw the word)\n",
    "        freq_dict[word] = 1\n",
    "    # If the word is already in the vocabulary, we increase the number with 1\n",
    "    else:\n",
    "        freq_dict[word] += 1\n",
    "\n",
    "# We can then see the frequency of each word\n",
    "print(freq_dict[\"of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has a tool that can count the frequency of the words much easier\n",
    "\n",
    "# First we create the frequency distribution with this command\n",
    "fd = FreqDist(manual_tok)\n",
    "\n",
    "# Then we can see which is the frequency of each word we are interested in\n",
    "# We got the same result as in the manual version\n",
    "print(fd[\"of\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see the most frequent words in the corpus using the most_common() method\n",
    "print(fd.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can observe the distribution of words on a plot\n",
    "fd.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "N-grams are important unit in NLP and CL\n",
    "\n",
    "An n-gram is a sequence of words of a predefined length\n",
    "\n",
    "The most common n-grams are bi-grams (two elements) and tri-grams (three elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has a function that calculates the bigrams from a list of tokens\n",
    "# Observe the following\n",
    "test_bigr = list(bigrams(manual_tok))\n",
    "\n",
    "print(test_bigr[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, instead of working with just one document, let's load 20 documents to have a little more text\n",
    "# Remember, if we don't give any file id, it loads the whole corpus. However working with the whole corpus can be slow.\n",
    "reuters_20 = reuters.raw(['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833', 'test/14839', 'test/14840', 'test/14841', 'test/14842', 'test/14843', 'test/14844', 'test/14849', 'test/14852', 'test/14854', 'test/14858', 'test/14859', 'test/14860', 'test/14861', 'test/14862', 'test/14863'])\n",
    "\n",
    "# Let's tokenize it using word_tokenize\n",
    "# N.B.: this might be take some time\n",
    "reuters_tok = word_tokenize(reuters_20)\n",
    "\n",
    "# Task 1\n",
    "# \n",
    "# Calculate the statistics for the whole corpus:\n",
    "# - number of tokens\n",
    "# - number of types\n",
    "# - the 20 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# \n",
    "# Calculate the number of bigrams in reuters_20\n",
    "# Calculate the number of unique bigrams in reuters_20\n",
    "# Print the 5 most frequent bigrams in reuters_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Topics\n",
    "\n",
    "Language models, regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced tasks\n",
    "\n",
    "# The following code reads a corpus of speeches of Donald Trump and generates a \"pseudo\" Trump sentences\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import codecs\n",
    "import codecs\n",
    "\n",
    "# Import random\n",
    "import random\n",
    "\n",
    "def get_markov_stats(trigrams):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        (list) trigrams\n",
    "    \n",
    "    Output: \n",
    "        (dict: key = str, value = list) \n",
    "            key: string of first two words in trigram  \n",
    "            value: list of all possible third words in trigram\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    markov_stats = {}\n",
    "    \n",
    "    for words in trigrams:\n",
    "        words_list = list(words)\n",
    "        word_12 = \" \".join([words_list[0], words_list[1]])\n",
    "        word_3 = words_list[2]\n",
    "        \n",
    "        # Check if there is an entry for the current word\n",
    "        if word_12 in markov_stats.keys():\n",
    "            # If it is, append the second one\n",
    "            markov_stats[word_12].append(word_3)\n",
    "\n",
    "        # If it isn't, create it with the corresponding value\n",
    "        else:\n",
    "            markov_stats[word_12] = [word_3]\n",
    "    return(markov_stats)\n",
    "\n",
    "def generate_sentence(corpus, stats):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        corpus: (list) corpus of words\n",
    "        stats: output of get_markov_stats\n",
    "    Output:\n",
    "        prints sentence according to rules given in assignment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get first two words of sentence, excluding .!?\n",
    "    first_bigram = list(random.choice(list(nltk.bigrams(corpus[:-1]))))\n",
    "    while \".\" in first_bigram or \"!\" in first_bigram or \"?\" in first_bigram or \",\" in first_bigram or \"’\" in first_bigram or first_bigram[0].islower():\n",
    "        first_bigram = list(random.choice(list(nltk.bigrams(corpus[:-1]))))\n",
    "    \n",
    "    new_speech = first_bigram\n",
    "\n",
    "    # Generate a sentence of minimum length 5\n",
    "    length = 1\n",
    "    while True:\n",
    "        # Get next word from previous two words\n",
    "        next_word = np.random.choice(stats[\" \".join(new_speech[-2:])])\n",
    "        \n",
    "        # Is sentence shorter than 5 words?\n",
    "        if length < 5: \n",
    "            # If it finds punctuation restart\n",
    "            if \".\" in next_word or \"!\" in next_word or \"?\" in next_word:\n",
    "                return(generate_sentence(corpus = corpus, stats = stats))  \n",
    "            # If no punctuation append word to existing sentence\n",
    "            else:\n",
    "                new_speech.append(next_word)\n",
    "                length += 1\n",
    "                \n",
    "        # Is sentence at least 5 words long?\n",
    "        elif length >= 5:\n",
    "            # If includes punctuation return\n",
    "            if \".\" in next_word or \"!\" in next_word or \"?\" in next_word:\n",
    "                new_speech.append(next_word)\n",
    "                return(\" \".join(new_speech))\n",
    "            # If no punctuation append word to existing sentence\n",
    "            else:\n",
    "                new_speech.append(next_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus\n",
    "raw_corpus = codecs.open('speeches.txt','r','utf8').read()\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Generate list of trigrams\n",
    "trump_trigr = list(nltk.trigrams(corpus))\n",
    "    \n",
    "# Generate model\n",
    "markov_stats = get_markov_stats(trump_trigr)\n",
    "   \n",
    "# Generate and print sentences\n",
    "for i in range(5):\n",
    "    sentence = generate_sentence(corpus, markov_stats)\n",
    "    print(sentence)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression tokenizer\n",
    "\n",
    "# The following code uses a regular espression grammar to process the corpus:\n",
    "\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "| \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "| \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "| \\.\\.\\.             # ellipsis\n",
    "| [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "'''\n",
    "\n",
    "regex_tok = regexp_tokenize(reuters_20,pattern)\n",
    "\n",
    "# Let's compare the regexp tokenizer and the word_tokenizer\n",
    "comp_tok(regex_tok,reuters_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 - advanced\n",
    "# Try to improve the regex tokenizer\n",
    "# Think of patterns that you want to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
